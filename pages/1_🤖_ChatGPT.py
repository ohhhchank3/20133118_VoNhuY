import os
import time

import openai
import pandas as pd
import streamlit as st

import bard_api as google
import bard_api_1 as config_bard
import demo4 as demo
import doctep as apikey1
import doctepdavinci as doctep
import gpt2_NLPHUST as gpt2hgf
import layDSModel_FineTune_davinci as dsdavinci
import layDSModel_FineTune_OpenAiAPI as dsfinetune
import LLam as llam
import LLam_vie as llamvi
import openAI_API_using_APIkey as openai_key
import openAI_API_using_Breaber_Token as openai_token
import openAI_API_usingAPI_text_davinci_003 as openai_text
import rapidapi1
import rapidapi2
import rapidapi3
import rapidapi_kethop2
import ViT5 as viT5
from utils import (load_prompt_templates, load_prompts, render_footer,
                   render_github_info)

st.session_state.sync_flag1 = ''
st.session_state.sync_flag2 = ''
st.session_state.sync_flag3 = ''
icon_path = r"https://raw.githubusercontent.com/ohhhchank3/hello/main/t%E1%BA%A3i%20xu%E1%BB%91ng.ico"
st.set_page_config(page_title="ChatBot Web", page_icon=icon_path)
import os

# G·ªôp l·∫°i hai ƒëi·ªÅu ki·ªán th√†nh m·ªôt d√≤ng
os.environ["keyOpenAI_dvc"], os.environ["originalID"] = doctep.main() if "keyOpenAI_dvc" not in os.environ or "originalID" not in os.environ else (os.environ.get("keyOpenAI_dvc", ""), os.environ.get("originalID", ""))
if "keyOpenAI" not in os.environ:
    os.environ["keyOpenAI"] = apikey1.main()
keyOpenAIdvc = ""
originalID = ""
keyOpenAIdvc = os.environ["keyOpenAI_dvc"]
originalID = os.environ["originalID"]
keyOpenAI = ""
keyOpenAI = "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"
flag1 = None
flag2 = None
flag3 = None

@st.cache_resource
def init_openai_settings():
    openai.api_key = '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'
    if st.secrets.get("OPENAI_PROXY"):
        openai.proxy = st.secrets["OPENAI_PROXY"]

def init_session():
    if not st.session_state.get("params"):
        st.session_state["params"] = dict()
    if not st.session_state.get("chats"):
        st.session_state["chats"] = {}
    if "input" not in st.session_state:
        st.session_state["input"] = "Hello, how are you!"

def new_chat(chat_name):
    if not st.session_state["chats"].get(chat_name):
        st.session_state["chats"][chat_name] = {
            "answer": [],
            "question": [],
            "messages": [
                {"role": "system", "content": st.session_state["params"]["prompt"]}
            ],
            "is_delete": False,
            "display_name": chat_name,
        }
    return chat_name

def switch_chat(chat_name):
    if st.session_state.get("current_chat") != chat_name:
        st.session_state["current_chat"] = chat_name
        render_chat(chat_name)
        st.stop()

def switch_chat_name(chat_name):
    if st.session_state.get("current_chat") != chat_name:
        st.session_state["current_chat"] = chat_name
        render_sidebar()
        render_chat(chat_name)
        st.stop()

def delete_chat(chat_name):
    if chat_name in st.session_state['chats']:
        st.session_state['chats'][chat_name]['is_delete'] = True
    current_chats = [chat for chat, value in st.session_state['chats'].items() if not value['is_delete']]
    if len(current_chats) == 0:
        switch_chat(new_chat(f"Chat{len(st.session_state['chats'])}"))
        st.stop()
    if st.session_state["current_chat"] == chat_name:
        del st.session_state["current_chat"]
        switch_chat_name(current_chats[0])

def edit_chat(chat_name, zone):
    def edit():
        if not st.session_state['edited_name']:
            print('T√™n b·ªã tr·ªëng!!')
            return None
        if (st.session_state['edited_name'] != chat_name
                and st.session_state['edited_name'] in st.session_state['chats']):
            print('T√™n b·ªã tr√πng l·∫∑p!!')
            return None
        if st.session_state['edited_name'] == chat_name:
            print('Kh√¥ng th·ªÉ thay ƒë·ªïi t√™n!!')
            return None
        st.session_state['chats'][chat_name]['display_name'] = st.session_state['edited_name']
    edit_zone = zone.empty()
    time.sleep(0.1)
    with edit_zone.container():
        st.text_input('T√™n m·ªõi', st.session_state['chats'][chat_name]['display_name'], key='edited_name')
        column1, _, column2 = st.columns([1, 5, 1])
        column1.button('‚úÖ', on_click=edit)
        column2.button('‚ùå')

def render_sidebar_chat_management(zone):
    new_chat_button = zone.button(label="‚ûï T·∫°o ƒëo·∫°n chat m·ªõi", use_container_width=True)
    if new_chat_button:
        new_chat_name = f"Chat{len(st.session_state['chats'])}"
        st.session_state["current_chat"] = new_chat_name
        new_chat(new_chat_name)

    with st.sidebar.container():
        for chat_name in st.session_state["chats"].keys():
            if st.session_state['chats'][chat_name]['is_delete']:
                continue
            if chat_name == st.session_state.get('current_chat'):
                column1, column2, column3 = zone.columns([7, 1, 1])
                column1.button(
                    label='üí¨ ' + st.session_state['chats'][chat_name]['display_name'],
                    on_click=switch_chat_name,
                    key=chat_name,
                    args=(chat_name,),
                    type='primary',
                    use_container_width=True,
                )
                column2.button(label='üìù', key='edit', on_click=edit_chat, args=(chat_name, zone))
                column3.button(label='üóëÔ∏è', key='remove', on_click=delete_chat, args=(chat_name,))
            else:
                zone.button(
                    label='üí¨ ' + st.session_state['chats'][chat_name]['display_name'],
                    on_click=switch_chat_name,
                    key=chat_name,
                    args=(chat_name,),
                    use_container_width=True,
                )
    if new_chat_button:
        switch_chat(new_chat_name)

def render_sidebar_rapidapi_config_tab(zone):
   st.session_state.sync_flag3 =  'hello'
   st.session_state["params"]["model_rapid"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn lo·∫°i m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng!!",
        ["Harleychatbot_translate","ChatGPT","BingChat","Lemurbot"],
        help="·ªû tr√™n l√† 3 m√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng trong Hugging Face",
    )

def render_sidebar_huggingface_config_tab(zone):
   st.session_state.sync_flag3 =  'hello'
   st.session_state["params"]["model_4"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn lo·∫°i m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng!!",
        ["Model LLama2","GPT2","ViT5"],
        help="·ªû tr√™n l√† 3 m√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng trong Hugging Face",
    )
   if st.session_state["params"]["model_4"] == "Model LLama2":
       st.session_state["params"]["model_llama_4"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn h√¨nh th·ª©c s·ª≠ d·ª•ng",
        ["LLama English","LLama VietNamese"],
        help="L·ª±a ch·ªçn h√¨nh th·ª©c s·ª≠ d·ª•ng c·ªßa model LLama", )

def render_sidebar_gpt_config_tab(zone):
   st.session_state.sync_flag3 =  'hello'
   prompt_text5 = zone.empty()
   st.session_state["params"] = dict()
   st.session_state["params"]["model"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn lo·∫°i m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng!!",
        ["Google-bard","ChatBot_openAPI","Model LLama"],
        help="N√™n s·ª≠ d·ª•ng ChatBot_openAPI ƒë·ªÉ d√πng c√°c d·ªãch v·ª• c·ªßa openAI, d√πng Google Bard ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh do Google t·∫°o ra,LLama l√† m·ªôt model m·ªõi g·∫ßn ƒë√¢y",
    )
   if st.session_state["params"]["model"] == "ChatBot_openAPI":
        st.session_state["params"]["model_openai"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn h√¨nh th·ª©c s·ª≠ d·ª•ng",
        ["Use_API_Key","Use_Breaber_Token","text-davinci-003_API"], 
        help="Ch·ªçn h√¨nh th·ª©c s·ª≠ d·ª•ng trong OpenAPI, d√πng API s·∫Ω s·ª≠ d·ª•ng API key v√† s·ª≠ d·ª•ng Breaber s·∫Ω d√πng m√£ token c√° nh√¢n,text-davinci-003 l√† m·ªôt m√¥ h√¨nh vƒÉn b·∫£n c·ªßa OpenAI",
      )
        if st.session_state["params"]["model_openai"] in ["Use_API_Key","Use_Breaber_Token"]:
            st.session_state["params"]["model_openAI_API"] = zone.selectbox(
                "Vui l√≤ng ch·ªçn Model mu·ªën s·ª≠ d·ª•ng trong OpenAI",
                ["gpt-3.5-turbo", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-1106", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-0301", "gpt-3.5-turbo-16k"],
                help="ID m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng, n√™n d√πng gpt-3.5-turbo-1106 ho·∫∑c gpt-3.5-turbo",
            ) 
            real_psid = st.session_state["params"].get("apikey3", "")
            st.session_state["params"]["apikey3"] = prompt_text5.text_input(
        "Nh·∫≠p m√£ API key",
        value='***' if real_psid else "None",
        key="input_psid1112",
        help="H√£y nh·∫≠p m√£ API key",
        type="password"  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
            st.session_state["params"]["temperature"] = zone.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.01,
        format="%0.2f",
        key ="tmpea",
        help="N√™n s·ª≠ d·ª•ng nhi·ªát ƒë·ªô l·∫•y m·∫´u n√†o, trong kho·∫£ng t·ª´ 0 ƒë·∫øn 2. C√°c gi√° tr·ªã cao h∆°n nh∆∞ 0,8 s·∫Ω l√†m cho ƒë·∫ßu ra ng·∫´u nhi√™n h∆°n, trong khi c√°c gi√° tr·ªã th·∫•p h∆°n nh∆∞ 0,2 s·∫Ω l√†m cho ƒë·∫ßu ra t·∫≠p trung v√† mang t√≠nh quy·∫øt ƒë·ªãnh h∆°n.",
    )
            st.session_state["params"]["max_tokens"] = zone.slider(
        "Max Token",
        value=2000,
        step=1,
        min_value=100,
        max_value=4096,
        key = "maxtoken",
        help="The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
    )
            st.session_state["params"]["presence_penalty"] = zone.slider(
        "Max Presence Penalty",
        value=0.0,
        step=0.01,
        min_value=-2.0,
        max_value=2.0,
        key ="pre",
        help="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    )
            st.session_state["params"]["frequency_penalty"] = zone.slider(
        "Max Frequency Penalty",
        value=0.0,
        step=0.01,
        min_value=-2.0,
        max_value=2.0,
        key = "fre",
        help="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
    )
            st.session_state["params"]["top_p"] = zone.slider(
        "Max Top P",
        value=0.7,
        step=0.01,
        min_value=0.0,
        max_value=1.0, key = "topp",
        help="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
    )
            st.session_state["params"]["stream"] = zone.checkbox(
        "Steaming output",
        value=True,
        key ="stream1",
        help="N·∫øu ƒë∆∞·ª£c ƒë·∫∑t, m·ªôt ph·∫ßn delta tin nh·∫Øn s·∫Ω ƒë∆∞·ª£c g·ª≠i, gi·ªëng nh∆∞ trong ChatGPT. M√£ th√¥ng b√°o s·∫Ω ƒë∆∞·ª£c g·ª≠i d∆∞·ªõi d·∫°ng s·ª± ki·ªán do m√°y ch·ªß g·ª≠i ch·ªâ d·ªØ li·ªáu khi ch√∫ng c√≥ s·∫µn, v·ªõi lu·ªìng ƒë∆∞·ª£c k·∫øt th√∫c b·∫±ng th√¥ng b√°o d·ªØ li·ªáu: [DONE]",
    )
            zone.caption('T√¨m ki·∫øm s·ª± gi√∫p ƒë·ª° t·∫°i https://platform.openai.com/docs/api-reference/chat')
   if "params" in st.session_state and st.session_state["params"].get("model_openai") in {"text-davinci-003_API"}:
     st.session_state["params"]["temperature1_1"] = zone.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.01,
        format="%0.2f",
        key="tmp12",
        help="N√™n s·ª≠ d·ª•ng nhi·ªát ƒë·ªô l·∫•y m·∫´u n√†o, trong kho·∫£ng t·ª´ 0 ƒë·∫øn 2. C√°c gi√° tr·ªã cao h∆°n nh∆∞ 0,8 s·∫Ω l√†m cho ƒë·∫ßu ra ng·∫´u nhi√™n h∆°n, trong khi c√°c gi√° tr·ªã th·∫•p h∆°n nh∆∞ 0,2 s·∫Ω l√†m cho ƒë·∫ßu ra t·∫≠p trung v√† mang t√≠nh quy·∫øt ƒë·ªãnh h∆°n.",
    )
     st.session_state["params"]["max_tokens1_1"] = zone.slider(
        "Max Token",
        value=2000,
        step=1,
        min_value=100,
        max_value=4096,
        key ="token12",
        help="The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
    )
     st.session_state["params"]["stop_1"] = zone.text_area(
        "T·ª´ Stop: ",
        "None",
        key="stop12",
        help="H√£y nh·∫≠p M√£ ID t·ªï ch·ª©c c·ªßa b·∫°n trong OpenAI Platform, ti·ªÅm ki·∫øm s·ª± tr·ª£ gi√∫p t·∫°i ƒë∆∞·ªùng d·∫´n https://platform.openai.com/account/organization",  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
     zone.caption('T√¨m ki·∫øm s·ª± gi√∫p ƒë·ª° t·∫°i https://platform.openai.com/docs/api-reference/chat')

def render_sidebar_using_model_finetune_config_tab(zone):
    prompt_text12 = zone.empty()
    st.session_state["params"]["model_2"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn lo·∫°i m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng!!",
        ["Finetune_XLM_Roberta","FineTune_OpenAI","FineTune_PhoBERT"],
        help="ƒê√¢y l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ do nh√≥m FineTune v·ªõi PhoBERT ki·∫øn tr√∫c base v√† XLM Roberta ki·∫øn tr√∫c base, OpenAI s·ª≠ d·ª•ng m√¥ h√¨nh gpt-3.5-turbo-1106 ho·∫∑c gpt-3.5-turbo-0613",
    )
    if st.session_state["params"]["model_2"] == "FineTune_OpenAI":
        st.session_state["params"]["model_openai_2"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn h√¨nh th·ª©c s·ª≠ d·ª•ng",
        ["gpt-3.5-turbo","text-davinci-002"],
        help="Ch·ªçn h√¨nh th·ª©c s·ª≠ d·ª•ng trong model FineTune",
      )
        if st.session_state["params"]["model_openai_2"] == "gpt-3.5-turbo":
            model_finetune = []
            model_finetune = dsfinetune.main()
            st.session_state["params"]["model_openAI_API_1"] = zone.selectbox(
                "Vui l√≤ng ch·ªçn Model mu·ªën s·ª≠ d·ª•ng trong FineTune OpenAI",
                model_finetune,
                help="ID m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng trong FineTune OpenAI",
            )
            st.session_state["params"]["temperature_2"] = zone.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.01,
        format="%0.2f",
        key = "1234",
        help="N√™n s·ª≠ d·ª•ng nhi·ªát ƒë·ªô l·∫•y m·∫´u n√†o, trong kho·∫£ng t·ª´ 0 ƒë·∫øn 2. C√°c gi√° tr·ªã cao h∆°n nh∆∞ 0,8 s·∫Ω l√†m cho ƒë·∫ßu ra ng·∫´u nhi√™n h∆°n, trong khi c√°c gi√° tr·ªã th·∫•p h∆°n nh∆∞ 0,2 s·∫Ω l√†m cho ƒë·∫ßu ra t·∫≠p trung v√† mang t√≠nh quy·∫øt ƒë·ªãnh h∆°n.",
    )
            st.session_state["params"]["max_tokens_2"] = zone.slider(
        "Max Token",
        value=1000,
        step=1,
        min_value=100,
        max_value=4096,
        key = "token2",
        help="The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
    )
            st.session_state["params"]["presence_penalty_2"] = zone.slider(
        "Max Presence Penalty",
        value=0.0,
        step=0.01,
        min_value=-2.0,
        max_value=2.0,
        help="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    )
            st.session_state["params"]["frequency_penalty_2"] = zone.slider(
        "Max Frequency Penalty",
        value=0.0,
        step=0.01,
        min_value=-2.0,
        max_value=2.0,
        help="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
    )
            st.session_state["params"]["top_p_2"] = zone.slider(
        "Max Top P",
        value=0.7,
        step=0.01,
        min_value=0.0,
        max_value=1.0,
        help="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
    )
            st.session_state["params"]["stream_2"] = zone.checkbox(
        "Steaming output",
        value=True,
        help="N·∫øu ƒë∆∞·ª£c ƒë·∫∑t, m·ªôt ph·∫ßn delta tin nh·∫Øn s·∫Ω ƒë∆∞·ª£c g·ª≠i, gi·ªëng nh∆∞ trong ChatGPT. M√£ th√¥ng b√°o s·∫Ω ƒë∆∞·ª£c g·ª≠i d∆∞·ªõi d·∫°ng s·ª± ki·ªán do m√°y ch·ªß g·ª≠i ch·ªâ d·ªØ li·ªáu khi ch√∫ng c√≥ s·∫µn, v·ªõi lu·ªìng ƒë∆∞·ª£c k·∫øt th√∫c b·∫±ng th√¥ng b√°o d·ªØ li·ªáu: [DONE]",
    )
            zone.caption('T√¨m ki·∫øm s·ª± gi√∫p ƒë·ª° t·∫°i https://platform.openai.com/docs/api-reference/chat')
    if "params" in st.session_state and st.session_state["params"].get("model_openai_2") in {"text-davinci-002"}:
      model_finetune = []
      model_finetune = dsdavinci.main()
      st.session_state["params"]["model_openAI_API_2"] = zone.selectbox(
                "Vui l√≤ng ch·ªçn Model mu·ªën s·ª≠ d·ª•ng trong OpenAI",
                model_finetune,
                help="ID m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng",
            )
      st.session_state["params"]["temperature_3"] = zone.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.01,
        format="%0.2f",
        key="tmp123",
        help="N√™n s·ª≠ d·ª•ng nhi·ªát ƒë·ªô l·∫•y m·∫´u n√†o, trong kho·∫£ng t·ª´ 0 ƒë·∫øn 2. C√°c gi√° tr·ªã cao h∆°n nh∆∞ 0,8 s·∫Ω l√†m cho ƒë·∫ßu ra ng·∫´u nhi√™n h∆°n, trong khi c√°c gi√° tr·ªã th·∫•p h∆°n nh∆∞ 0,2 s·∫Ω l√†m cho ƒë·∫ßu ra t·∫≠p trung v√† mang t√≠nh quy·∫øt ƒë·ªãnh h∆°n.",
    )
      st.session_state["params"]["max_tokens_3"] = zone.slider(
        "Max Token",
        value=2000,
        step=1,
        min_value=100,
        max_value=4096,
        key ="token123",
        help="The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
    )
      st.session_state["params"]["stop_2"] = zone.text_area(
        "T·ª´ Stop: ",
        "None",
        key="stop123",
        help="H√£y nh·∫≠p M√£ ID t·ªï ch·ª©c c·ªßa b·∫°n trong OpenAI Platform, ti·ªÅm ki·∫øm s·ª± tr·ª£ gi√∫p t·∫°i ƒë∆∞·ªùng d·∫´n https://platform.openai.com/account/organization",  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
      zone.caption('T√¨m ki·∫øm s·ª± gi√∫p ƒë·ª° t·∫°i https://platform.openai.com/docs/api-reference/chat')
    if st.session_state["params"]["model_2"] == "Finetune_XLM_Roberta":
        st.session_state["params"]["text2"] = prompt_text12.text_area(
        "H√£y nh·∫≠p ƒëo·∫°n vƒÉn: ",
        "None",
        key="doanvan123",
        help="H√£y nh·∫≠p ƒëo·∫°n vƒÉn c·ªßa b·∫°n v√†o",
    )
    if st.session_state["params"]["model_2"] == "FineTune_PhoBERT":
        st.session_state["params"]["model_phobert"] = zone.selectbox(
        "Vui l√≤ng ch·ªçn phi√™n b·∫£n FineTune PhoBERT b·∫°n s·ª≠ d·ª•ng!!",
        ["Version","FineTune 32 batch","FineTune 64 batch"],
        help="ƒê√¢y l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ do nh√≥m FineTune",key = "hello123",)
        if st.session_state["params"]["model_phobert"] == "Version":
              st.session_state["params"]["model_phobert_version"] =zone.selectbox(
              "Vui l√≤ng ch·ªçn phi√™n b·∫£n Version b·∫°n s·ª≠ d·ª•ng!!",
              ["Version1","Version2","Version3","Version4"],
              help="ƒê√¢y l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ do nh√≥m FineTune",key = "pbversion",)
        if st.session_state["params"]["model_phobert"] == "FineTune 32 batch":
              st.session_state["params"]["model_phobert_32"] = zone.selectbox(
              "Vui l√≤ng ch·ªçn phi√™n b·∫£n FineTune c√≥ k√≠ch th∆∞·ªõc batch 32 b·∫°n s·ª≠ d·ª•ng!!",
              ["Model_32_batch 50_epoch","Model_32_batch 100_epoch","Model_32_batch 150_epoch"],
              help="ƒê√¢y l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ do nh√≥m FineTune v·ªõi k√≠ch th∆∞·ªõc batch size l√† 32 v√† s·ªë l·∫ßn l·∫∑p t·ª´ 50 ƒë·∫øn 150",key = "pbbatch32",)
        if st.session_state["params"]["model_phobert"] == "FineTune 64 batch":
               st.session_state["params"]["model_phobert_64"] = zone.selectbox(
               "Vui l√≤ng ch·ªçn phi√™n b·∫£n FineTune c√≥ k√≠ch th∆∞·ªõc Batch 64 b·∫°n s·ª≠ d·ª•ng!!",
               ["Model_64_batch 50_epoch","Model_64_batch 100_epoch"],
               help="ƒê√¢y l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ do nh√≥m FineTune v·ªõi k√≠ch th∆∞·ªõc batch size l√† 64 v√† s·ªë l·∫ßn l·∫∑p t·ª´ 50 ƒë·∫øn 100",key = "pbbatch64",)
   
def render_sidebar_gpt_using_my_key_config_tab(zone):
    st.session_state.sync_flag1 =  'gptmykey'
    prompt_text5 = zone.empty()
    prompt_text6 = zone.empty()
    prompt_text8 = zone.empty()
    prompt_text9 = zone.empty()
    prompt_text10 = zone.empty()
    prompt_text11 = zone.empty()
    real_apikey = st.session_state["params"].get("api_key1", "")
    real_id = st.session_state["params"].get("original_ID", "")
    prompt_text7 = zone.empty()
    st.session_state["params"]["api_key1"] = zone.text_input(
        "Kh√≥a API c·ªßa b·∫°n:",
        value='***' if real_apikey else "None",
        key="input_apikey",
        help="H√£y nh·∫≠p m√£ API Key c·ªßa b·∫°n trong OpenAI Platform, L·∫•y th√¥ng tin API key t·∫°i https://platform.openai.com/api-keys",
        type="password"  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
    st.session_state["params"]["model_key"] = prompt_text5.selectbox(
        "Ch·ªçn M√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng: ",
        ["Use_API_Key", "text-davinci-003_API"],
        help="ID m√¥ h√¨nh b·∫°n s·ª≠ d·ª•ng trong OpenAI s·ª≠ d·ª•ng API key s·∫Ω d√πng v√†o m√¥ h√¨nh gpt-3.5-turbo tr·ªü l√™n",
    )
    if st.session_state["params"]["model_key"] in {'Use_API_Key'}:
     st.session_state["params"]["model_key_openAI"] = prompt_text6.selectbox(
            "Vui l√≤ng ch·ªçn model s·ª≠ d·ª•ng trong OpenAI API",
            ["gpt-3.5-turbo-1106", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-0301", "gpt-3.5-turbo-16k"],
            help="ID c·ªßa model b·∫°n s·ª≠ d·ª•ng khuy·∫øn ngh·ªã s·ª≠ d·ª•ng gpt-3.5-turbo-1106, th√¥ng tin chi ti·∫øt t·∫°i https://platform.openai.com/docs/models",
            key ="typeModel"
        )
    if st.session_state["params"]["model_key"] in {"text-davinci-003_API"}:
     st.session_state["params"]["original_ID"] = prompt_text8.text_input(
        "M√£ ID t·ªï ch·ª©c c·ªßa b·∫°n: ",
        value='***' if real_id else "None",
        key="input_apiid",
        help="H√£y nh·∫≠p M√£ ID t·ªï ch·ª©c c·ªßa b·∫°n trong OpenAI Platform, ti·ªÅm ki·∫øm s·ª± tr·ª£ gi√∫p t·∫°i ƒë∆∞·ªùng d·∫´n https://platform.openai.com/account/organization",
        type="password"  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
     st.session_state["params"]["temperature1"] = prompt_text9.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.01,
        format="%0.2f",
        key="tmp1",
        help="N√™n s·ª≠ d·ª•ng nhi·ªát ƒë·ªô l·∫•y m·∫´u n√†o, trong kho·∫£ng t·ª´ 0 ƒë·∫øn 2. C√°c gi√° tr·ªã cao h∆°n nh∆∞ 0,8 s·∫Ω l√†m cho ƒë·∫ßu ra ng·∫´u nhi√™n h∆°n, trong khi c√°c gi√° tr·ªã th·∫•p h∆°n nh∆∞ 0,2 s·∫Ω l√†m cho ƒë·∫ßu ra t·∫≠p trung v√† mang t√≠nh quy·∫øt ƒë·ªãnh h∆°n.",
    )
     st.session_state["params"]["max_tokens1"] = prompt_text10.slider(
        "Max Token",
        value=2000,
        step=1,
        min_value=100,
        max_value=4096,
        key ="token1",
        help="The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
    )
     st.session_state["params"]["stop"] = prompt_text11.text_area(
        "T·ª´ Stop: ",
        "None",
        key="stop1",
        help="H√£y nh·∫≠p M√£ ID t·ªï ch·ª©c c·ªßa b·∫°n trong OpenAI Platform, ti·ªÅm ki·∫øm s·ª± tr·ª£ gi√∫p t·∫°i ƒë∆∞·ªùng d·∫´n https://platform.openai.com/account/organization",  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
    zone.caption('T√¨m ki·∫øm s·ª± tr·ª£ gi√∫p t·∫°i https://platform.openai.com/docs/api-reference/chat')

def render_sidebar_prompt_config_tab(zone):
    prompt_text = zone.empty()
    st.session_state["params"]["prompt"] = prompt_text.text_area(
        "Ph·∫£n h·ªìi l·ªùi nh·∫Øc",
        "You are a helpful assistant that answer questions as possible as you can.",
        help="The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.",
    )
    template = zone.selectbox('Loading From Prompt Template', load_prompt_templates())
    if template:
        prompts_df = load_prompts(template)
        actor = zone.selectbox('ƒêang t·∫£i l·ªùi nh·∫Øc', prompts_df.index.tolist())
        if actor:
            st.session_state["params"]["prompt"] = prompt_text.text_area(
                "H·ªá th·ªëng l·ªùi nh·∫Øc",
                prompts_df.loc[actor].prompt,
                help="The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.",
            )
import streamlit as st


def render_sidebar_google_bard_config_tab(zone):
    st.session_state.sync_flag2 = 'bard'
    # S·ª≠ d·ª•ng bi·∫øn trung gian ƒë·ªÉ l∆∞u gi·ªØ gi√° tr·ªã th·ª±c v√† gi√° tr·ªã ·∫©n
    real_psid = st.session_state["params"].get("1_PSID", "")
    real_psidcc = st.session_state["params"].get("1_PSIDCC", "")
    real_psidts = st.session_state["params"].get("1_PSIDTS", "")
    st.session_state["params"]["1_PSID"] = zone.text_input(
        "Nh·∫≠p m√£ Secure 1_PSID",
        value='***' if real_psid else "None",
        key="input_psid",
        help="H√£y nh·∫≠p m√£ Cookie Secure 1_PSID, h√£y v√†o trang https://bard.google.com/chat ch·ªçn F12 ch·ªçn Application v√† ch·ªçn c√°c Cookie c·∫ßn thi·∫øt!",
        type="password"  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
    st.session_state["params"]["1_PSIDCC"] = zone.text_input(
        "Nh·∫≠p m√£ Secure 1_PSIDCC",
        value='***' if real_psidcc else "None",
        key="input_psidcc",
        help="Secure 1_PSIDCC, h√£y v√†o trang https://bard.google.com/chat ch·ªçn F12 ch·ªçn Application v√† ch·ªçn c√°c Cookie c·∫ßn thi·∫øt!",
        type="password"  # ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
    st.session_state["params"]["1_PSIDTS"] = zone.text_input(
        "Nh·∫≠p m√£ Secure 1_PSIDTS",
        value='***' if real_psidts else "None",
        key="input_psidts",
        help="Secure 1_PSIDTS, h√£y v√†o trang https://bard.google.com/chat ch·ªçn F12 ch·ªçn Application v√† ch·ªçn c√°c Cookie c·∫ßn thi·∫øt!",
        type="password"# ƒê·∫∑t ki·ªÉu d·ªØ li·ªáu l√† password ƒë·ªÉ ·∫©n gi√° tr·ªã nh·∫≠p v√†o
    )
# S·ª≠ d·ª•ng h√†m
def render_download_zone(zone):
    from io import BytesIO, StringIO
    if not st.session_state.get('current_chat'):
        return
    chat = st.session_state['chats'][st.session_state['current_chat']]
    col1, col2 = zone.columns([1, 1])
    chat_messages = ['# ' + chat['display_name']]
    if chat["question"]:
        for i in range(len(chat["question"])):
            chat_messages.append(f"""üíé **YOU:** {chat["question"][i]}""")
            if i < len(chat["answer"]):
                chat_messages.append(f"""ü§ñ **AI:** {chat["answer"][i]}""")
        col1.download_button('üì§ Markdown', '\n'.join(chat_messages).encode('utf-8'),
                             file_name=f"{chat['display_name']}.md", help="Download messages to a markdown file",
                             use_container_width=True)
    tables = []
    for answer in chat["answer"]:
        filter_table_str = '\n'.join([m.strip() for m in answer.split('\n') if m.strip().startswith('| ') or m == ''])
        try:
            tables.extend(
                [pd.read_table(StringIO(filter_table_str.replace(' ', '')), sep='|').dropna(axis=1, how='all').iloc[1:]
                 for m in filter_table_str.split('\n\n')])
        except Exception as e:
            print(e)
    if tables:
        buffer = BytesIO()
        with pd.ExcelWriter(buffer) as writer:
            for index, table in enumerate(tables):
                table.to_excel(writer, sheet_name=str(index + 1), index=False)
        col2.download_button('üìâ Excel', buffer.getvalue(), file_name=f"{chat['display_name']}.xlsx",
                             help="Download tables to a excel file", use_container_width=True)
selected_tab = None

def get_session():
    if "selected_tab" not in st.session_state:
        st.session_state.selected_tab = 'ChatGPT'
    if "prompt_checked" not in st.session_state:
        st.session_state.prompt_checked = False
    if "bard_checked" not in st.session_state:
        st.session_state.bard_checked = False
    if "apikey_checked" not in st.session_state:
        st.session_state.apikey_checked = False
    if "gpt_checked" not in st.session_state:
        st.session_state.gpt_checked = False
    if "finetune_checked" not in st.session_state:
        st.session_state.finetune_checked = False
    if "huggingface_checked" not in st.session_state:
        st.session_state.huggingface_checked = False
    if "rapidapi_checked" not in st.session_state:
        st.session_state.rapidapi_checked = False
    return st.session_state

def render_sidebar():
    # Initialize session state
    session_state = get_session()
    # Set up the sidebar components
    chat_name_container = st.sidebar.container()
    chat_config_expander = st.sidebar.expander('‚öôÔ∏è C·∫•u h√¨nh Chat', True)
    tab_gpt,tab_rapid, tab_prompt, tab_bard, chatgpt_mykey,huggingface = chat_config_expander.tabs(
        ['üåê  ChatBot','‚ùÑÔ∏è Rapid API','üë• H·ªôp tho·∫°i g·ª£i √Ω', 'üåè  Google Bard', 'üìö  ChatGPT use APIKey',"ü§ó Hugging Face"]
    )
    download_zone = st.sidebar.empty()
    github_zone = st.sidebar.empty()
    # Render the content of each tab
    render_sidebar_gpt_config_tab(tab_gpt)
    render_sidebar_rapidapi_config_tab(tab_rapid)
    render_sidebar_prompt_config_tab(tab_prompt)
    render_sidebar_google_bard_config_tab(tab_bard)
    render_sidebar_gpt_using_my_key_config_tab(chatgpt_mykey)
   # render_sidebar_using_model_finetune_config_tab(model_finetune)
    render_sidebar_huggingface_config_tab(huggingface)
    render_sidebar_chat_management(chat_name_container)
    render_download_zone(download_zone)
    render_github_info(github_zone)
    # G√°n gi√° tr·ªã cho selected_tab khi kh√¥ng nh·∫•n v√†o button
    if tab_gpt.checkbox('üåê ChatGPT', value=(session_state.selected_tab == 'ChatGPT')):
        session_state.selected_tab = 'ChatGPT'
        session_state.prompt_checked = False
        session_state.bard_checked = False
        session_state.apikey_checked = False
        session_state.finetune_checked = False
        session_state.huggingface_checked = False
        session_state.rapidapi_checked = False
        st.write(f"ƒê√£ ch·ªçn tab: {session_state.selected_tab}")
    if tab_prompt.checkbox('üë• Prompt', value=(session_state.selected_tab == 'Prompt')):
        session_state.selected_tab = 'Prompt'
        session_state.gpt_checked = False
        session_state.bard_checked = False
        session_state.apikey_checked = False
        session_state.finetune_checked = False
        session_state.huggingface_checked = False
        session_state.rapidapi_checked = False
        st.write(f"ƒê√£ ch·ªçn tab: {session_state.selected_tab}")
    if tab_bard.checkbox('üåè Google Bard', value=(session_state.selected_tab == 'Google Bard')):
        session_state.selected_tab = 'Google Bard'
        session_state.gpt_checked = False
        session_state.prompt_checked = False
        session_state.apikey_checked = False
        session_state.finetune_checked = False
        session_state.rapidapi_checked = False
        session_state.huggingface_checked = False
        st.write(f"ƒê√£ ch·ªçn tab: {session_state.selected_tab}")
    if chatgpt_mykey.checkbox('üìö ChatGPT APIKey', value=(session_state.selected_tab == 'ChatGPT APIKey')):
        session_state.selected_tab = 'ChatGPT APIKey'
        session_state.gpt_checked = False
        session_state.prompt_checked = False
        session_state.rapidapi_checked = False
        session_state.bard_checked = False
        session_state.finetune_checked = False
        session_state.huggingface_checked = False
        st.write(f"ƒê√£ ch·ªçn tab: {session_state.selected_tab}")
    if huggingface.checkbox('ü§ó Hugging Face', value=(session_state.selected_tab == 'HuggingFace')):
        session_state.selected_tab = 'HuggingFace'
        session_state.gpt_checked = False
        session_state.prompt_checked = False
        session_state.bard_checked = False
        session_state.rapidapi_checked = False
        session_state.apikey_checked = False
        session_state.finetune_checked = False
        st.write(f"ƒê√£ ch·ªçn tab: {session_state.selected_tab}")
    if tab_rapid.checkbox('‚ùÑÔ∏è Rapid API', value=(session_state.selected_tab == 'Rapid API')):
        session_state.selected_tab = 'Rapid API'
        session_state.gpt_checked = False
        session_state.prompt_checked = False
        session_state.bard_checked = False
        session_state.huggingface_checked = False
        session_state.apikey_checked = False
        session_state.finetune_checked = False
        st.write(f"ƒê√£ ch·ªçn tab: {session_state.selected_tab}")

def render_user_message(message, zone):
    col1, col2 = zone.columns([1, 8])
    col1.markdown("üëª **B·∫°n:**")
    col2.markdown(message)

def render_ai_message(message, zone):
    col1, col2 = zone.columns([1, 8])
    col1.markdown("ü§ñ **Chat:**")
    col2.markdown(message)

def render_history_answer(chat, zone):
    zone.empty()
    time.sleep(0.1)
    with zone.container():
        if chat['messages']:
            st.caption(f"""‚ÑπÔ∏è Prompt: {chat["messages"][0]['content']}""")
        if chat["question"]:
            for i in range(len(chat["question"])):
                render_user_message(chat["question"][i], st)
                if i < len(chat["answer"]):
                    render_ai_message(chat["answer"][i], st)

def render_last_answer7(question,chat,zone):
    answer_zone = zone.empty()
    chat["messages"].append(question)
    chat["question"].append(question)
    response = ""
    question = ' '.join(question.split())
    if st.session_state["params"]["model_rapid"] in {'Harleychatbot_translate'}:
       with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        response = rapidapi_kethop2.main(question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
    if st.session_state["params"]["model_rapid"] in {'ChatGPT'}:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        processed_question = ' '.join(question.split())
        response = rapidapi1.main(processed_question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    if st.session_state["params"]["model_rapid"] in {'BingChat'}:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        processed_question = ' '.join(question.split())
        response = rapidapi2.get_bingchat_response(processed_question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    if st.session_state["params"]["model_rapid"] in {'Lemurbot'}:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        processed_question = ' '.join(question.split())
        response = rapidapi3.get_chatbot_response(processed_question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)

def render_last_answer6(question,chat,zone):
    answer_zone = zone.empty()
    chat["messages"].append(question)
    chat["question"].append(question)
    response = ""
    question = ' '.join(question.split())
    if st.session_state["params"]["model_4"] in {'Model LLama2'}:
      if st.session_state["params"]["model_llama_4"] in {'LLama English'}:
       with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        response = llam.get_assistant_response(question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
      elif st.session_state["params"]["model_llama_4"] in {'LLama VietNamese'}:
       with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        response = llamvi.get_assistant_response(question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
    elif st.session_state["params"]["model_4"] in {'GPT2'}:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        processed_question = ' '.join(question.split())
        response = gpt2hgf.generate_text(processed_question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    else:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        processed_question = ' '.join(question.split())
        response = viT5.summarize_text(processed_question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    


def render_last_answer4(question,chat,zone):
    answer_zone = zone.empty()
    chat["messages"].append(question)
    chat["question"].append(question)
    response = ""
    if st.session_state["params"]["model_key"] in {'Use_API_Key'}:
         with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
          key = ""
          key = st.session_state["params"]["api_key1"]
          if key == None or len(key) == 0:
              key = keyOpenAI
          model_value = st.session_state["params"]["model_key_openAI"]
          response = get_openai_response_api_key(question,model_value,key)
          answer = ""
          answer = response
          chat["answer"].append(answer)
          chat["messages"].append({"role": "assistant", "content": answer})
          render_ai_message(answer, answer_zone)
    elif st.session_state["params"]["model_key"] in {'text-davinci-003_API'}:
         with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
          key = ''
          key = st.session_state["params"]["api_key1"]
          if key == None or len(key) == 0:
              key = keyOpenAI
          token = st.session_state["params"]["max_tokens1"]
          temp = st.session_state["params"]["temperature1"]
          stp = ""
          stp = st.session_state["params"]["stop"]
          oriID = st.session_state["params"]["original_ID"]
          response = get_openai_response_text_davinci(question,token,key,oriID,temp,stp)
          answer = ""
          answer = response
          chat["answer"].append(answer)
          chat["messages"].append({"role": "assistant", "content": answer})
          render_ai_message(answer, answer_zone)

def render_last_answer3(question,chat,zone):
        with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
         flag2 = False
         chat["messages"].append(question)
         chat["question"].append(question)
         response = ''
         answer_zone = zone.empty()
         psid =st.session_state["params"]["1_PSID"]
         psidts =st.session_state["params"]["1_PSIDTS"]
         psidcc =st.session_state["params"]["1_PSIDCC"]
         answer = ""
         bard1 = config_bard.initialize_bard_session(psid,psidts,psidcc)
         response = config_bard.send_message(question)
         answer = response
         chat["answer"].append(answer)
         chat["messages"].append({"role": "assistant", "content": answer})
         render_ai_message(answer, answer_zone)

def render_last_answer2(question, chat, zone):
    answer_zone = zone.empty()
    chat["messages"].append(question)
    chat["question"].append(question)
    response = ""
    question = ' '.join(question.split())
    if st.session_state["params"]["model"] in {'chatbot-fake'}:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        response = get_openai_response2(question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    elif st.session_state["params"]["model"] in {'Google-bard'}:
      with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        response = get_openai_response3(question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    elif st.session_state["params"]["model"] in {'gpt-3.5-turbo','gpt-3.5-turbo-1106'}:
       with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        model_value = st.session_state["params"]["model"]
        temperature=st.session_state["params"]["temperature"]
        fre=st.session_state["params"]["frequency_penalty"]
        pre = st.session_state["params"]["presence_penalty"]
        max_tokens=st.session_state["params"]["max_tokens"]
        top_p=st.session_state["params"]["top_p"]
        response = get_openai_response1(question, model_value,temperature,pre,fre,max_tokens,top_p)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    elif st.session_state["params"]["model"] in {'Model LLama'}:
     with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
        response = llam.get_assistant_response(question)
        answer = ""
        answer = response
        chat["answer"].append(answer)
        chat["messages"].append({"role": "assistant", "content": answer})
        render_ai_message(answer, answer_zone)
    elif st.session_state["params"]["model"] in {'ChatBot_openAPI'}:
        if st.session_state["params"]["model_openai"] in {'Use_API_Key'}:
         with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
          apikey = st.session_state["params"]["apikey3"] 
          model_value = st.session_state["params"]["model_openAI_API"]
          response = get_openai_response_api_key(question,model_value,apikey)
          answer = ""
          answer = response
          chat["answer"].append(answer)
          chat["messages"].append({"role": "assistant", "content": answer})
          render_ai_message(answer, answer_zone)
        elif st.session_state["params"]["model_openai"] in {'Use_Breaber_Token'}:
         with st.spinner("Ch·ªù ph·∫£n h·ªïi..."):
          model_value = st.session_state["params"]["model_openAI_API"]
          response = get_openai_response_api_token(question,model_value)
          answer = ""
          answer = response
          chat["answer"].append(answer)
          chat["messages"].append({"role": "assistant", "content": answer})
          render_ai_message(answer, answer_zone)
        else:
         with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
          token = st.session_state["params"]["max_tokens1_1"]
          temp = st.session_state["params"]["temperature1_1"]
          stp = ""
          stp = st.session_state["params"]["stop_1"]
          response = get_openai_response_text_davinci(question,token,keyOpenAIdvc,originalID,temp,stp)
          answer = ""
          answer = response
          chat["answer"].append(answer)
          chat["messages"].append({"role": "assistant", "content": answer})
          render_ai_message(answer, answer_zone)
    else:
        if st.session_state["params"]["stream"]:
            with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
                model_value = st.session_state["params"]["model"]
                temperature=st.session_state["params"]["temperature"]
                fre=st.session_state["params"]["frequency_penalty"]
                pre = st.session_state["params"]["presence_penalty"]
                max_tokens=st.session_state["params"]["max_tokens"]
                top_p=st.session_state["params"]["top_p"]
                answer = ""
                response = get_openai_response1(question, model_value,temperature,pre,fre,max_tokens,top_p)
                answer = response
                chat["answer"].append(answer)
                chat["messages"].append({"role": "assistant", "content": answer})
                render_ai_message(answer, answer_zone)
        else:
            with st.spinner("Ch·ªù ph·∫£n h·ªìi..."):
                model_value = st.session_state["params"]["model"]
                temperature=st.session_state["params"]["temperature"]
                fre=st.session_state["params"]["frequency_penalty"]
                pre = st.session_state["params"]["presence_penalty"]
                max_tokens=st.session_state["params"]["max_tokens"]
                top_p=st.session_state["params"]["top_p"]
                answer = ""
                response = get_openai_response1(question, model_value,temperature,pre,fre,max_tokens,top_p)
                answer = response
                chat["answer"].append(answer)
                chat["messages"].append({"role": "assistant", "content": answer})
                render_ai_message(answer, zone)

# Call the function
def render_stop_generate_button(zone):
    def stop():
        st.session_state['regenerate'] = False
    zone.columns((2, 1, 2))[1].button('‚ñ° D·ª´ng', on_click=stop)

def render_regenerate_button(chat, zone):
    def regenerate():
        chat["messages"].pop(-1)
        chat["messages"].pop(-1)
        chat["answer"].pop(-1)
        st.session_state['regenerate'] = True
        st.session_state['last_question'] = chat["question"].pop(-1)
    zone.columns((2, 1, 2))[1].button('üîÑT·∫°o l·∫°i', type='primary', on_click=regenerate)

def render_chat(chat_name):
    def handle_ask():
        if st.session_state['input']:
            re_generate_zone.empty()
            render_user_message(st.session_state['input'], last_question_zone)
            render_stop_generate_button(stop_generate_zone)
            if st.session_state.selected_tab in ['ChatGPT','Prompt'] :
                render_last_answer2(st.session_state['input'], chat, last_answer_zone)
            if st.session_state.selected_tab == 'Google Bard' and st.session_state["params"]["1_PSID"] not in["None"]:
                render_last_answer3(st.session_state["input"],chat,last_answer_zone)
            if st.session_state.selected_tab == 'ChatGPT APIKey' and st.session_state["params"]["api_key1"]not in["None"]:
                render_last_answer4(st.session_state["input"],chat,last_answer_zone)
            if st.session_state.selected_tab == 'HuggingFace':
                render_last_answer6(st.session_state["input"],chat,last_answer_zone)
            if st.session_state.selected_tab == 'Rapid API':
                render_last_answer7(st.session_state["input"],chat,last_answer_zone)
            st.session_state['input'] = ''
    if chat_name not in st.session_state["chats"]:
        st.error(f'{chat_name} is not exist')
        return
    chat = st.session_state["chats"][chat_name]
    if chat['is_delete']:
        st.error(f"{chat_name} is deleted")
        st.stop()
    if len(chat['messages']) == 1 and st.session_state["params"]["prompt"]:
        chat["messages"][0]['content'] = st.session_state["params"]["prompt"]
    conversation_zone = st.container()
    history_zone = conversation_zone.empty()
    last_question_zone = conversation_zone.empty()
    last_answer_zone = conversation_zone.empty()
    ask_form_zone = st.empty()
    render_history_answer(chat, history_zone)
    ask_form = ask_form_zone.form(chat_name)
    col1, col2 = ask_form.columns([10, 1])
    col1.text_area("üëª B·∫°n: ",
                   key="input",
                   max_chars=4000,
                   label_visibility='collapsed')
    with col2.container():
        for _ in range(2):
            st.write('\n')
        st.form_submit_button("üöÄ", on_click=handle_ask)
    stop_generate_zone = conversation_zone.empty()
    re_generate_zone = conversation_zone.empty()
    if st.session_state.get('regenerate'):
        render_user_message(st.session_state['last_question'], last_question_zone)
        render_stop_generate_button(stop_generate_zone)
        if st.session_state.selected_tab in ['ChatGPT','Prompt'] :
                render_last_answer2(st.session_state['last_question'], chat, last_answer_zone)
        if st.session_state.selected_tab == 'Google Bard' and st.session_state["params"]["1_PSID"] not in["None"]:
                render_last_answer3(st.session_state["last_question"],chat,last_answer_zone)
        if st.session_state.selected_tab == 'ChatGPT APIKey' and st.session_state["params"]["api_key1"]not in["None"]:
                render_last_answer4(st.session_state["last_question"],chat,last_answer_zone)
        if st.session_state.selected_tab == 'HuggingFace':
                render_last_answer6(st.session_state["input"],chat,last_answer_zone)
        if st.session_state.selected_tab == 'Rapid API':
                render_last_answer7(st.session_state["input"],chat,last_answer_zone)
        st.session_state['regenerate'] = False
    if chat["answer"]:
        stop_generate_zone.empty()
        render_regenerate_button(chat, re_generate_zone)
    render_footer()

def get_openai_response1(messages,model,temperature,pre,fre,token,top_p):
    response = demo.get_chat_completion(messages,model,temperature,pre,fre,token,top_p)
    return response
def get_openai_response2(messages):
    return None
def get_openai_response3(messages):
    response = google.send_message(messages)
    return response
def get_openai_response_api_key(messages,model,apikey):
    response = openai_key.main(messages,model,apikey)
    return response
def get_openai_response_api_token(messages,model):
    response = openai_token.main(messages,model)
    return response
def get_openai_response_text_davinci(messages,token,keyOpenAIdvc,originalID,temp,stop):
    response = openai_text.main(messages,token,keyOpenAIdvc,originalID,temp,stop)
    return response


def get_openai_response(messages):
    if st.session_state["params"]["model"] in {'gpt-3.5-turbo', 'gpt4','text-davinci-002-render-sha','gpt-3.5-turbo-1106'}:
        response = openai.ChatCompletion.create(
            model=st.session_state["params"]["model"],
            temperature=st.session_state["params"]["temperature"],
            messages=messages,
            stream=st.session_state["params"]["stream"],
            max_tokens=st.session_state["params"]["max_tokens"],
        )
    else:
        raise NotImplementedError('Not implemented yet!')
    return response

if __name__ == "__main__":
    init_openai_settings()
    init_session()
    render_sidebar()
    if st.session_state.get("current_chat"):
        render_chat(st.session_state["current_chat"])
    if len(st.session_state["chats"]) == 0:
        switch_chat(new_chat(f"Chat{len(st.session_state['chats'])}"))
